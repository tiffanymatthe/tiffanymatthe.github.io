---
title: Connect 4 AI
image: connect.png
tags:
  - Python
  - TensorFlow
color: db9696
titleColor: c75d5d
description: A Connect 4 AI that was trained using a similar technique for AlphaZero by DeepMind.
year: 2021
---
[connect-four - Github](https://github.com/tiffanymatthe/connect-four)

To gain a better understanding of reinforcement learning and DeepMind’s AlphaGo Zero, we set out to train a neural network to play the game Connect 4.

<aside>
Connect 4 is a two-player game where each player attempts to create a connected line of 4 of their tokens on a vertical 6 x 7 board. The game ends when one player makes the line or when the board is filled up. A move is made by placing a token in one of the seven columns.
</aside>

## Summary

To train the deep residual neural network using reinforcement learning, I adopted the strategy used to train AlphaGo Zero[^1] and AlphaZero[^2] by DeepMind, which bypasses the need for external training data. Instead, training data is created through self-play games whose moves are guided by the neural network. The neural network will train on these games, and then more training data will be generated using the newly trained neural network. The new training data will help the neural network train again. This iterative process will continue until the model reaches a satisfactory intelligence.

In the beginning, the moves chosen in self-play games will be at random since the model is not well-trained, but as we go through the training iterations, the model used in the games will become more and more intelligent, thus giving the next training round better data to train on.

The code for the project can be found on [Github](https://github.com/tiffanymatthe/connect-four). It is still a work in progress due to limited infrastructure and time.

## Self-Play Games

Each game is played using Monte Carlo Tree Search (MCTS) and the latest neural network. MCTS is used to pick the next “best” move in the game. The algorithm is as follows:

1. **Selection**. Traverse down expanded nodes by selecting those with the greatest UCT score. The score is computed as shown in Czech et al. 2020 [^3].
    
    ```python
    c_puct_base = 19652
    c_puct_init = 2.5 # based on Silver 2017
    c_puct = math.log((parent_node.visit_count + c_pct_base + 1) / c_puct_base) + c_puct_init
    # scaled with respect to the number of visits of a particular node
    uct = c_puct * math.sqrt(math.log(parent_node.visit_count) / child_node.visit_count)
    ```
    
2. **Expansion.** Once an unexpanded node is reached, expand it into its children.
    1. Each child is the same game state as its parent but with one extra action taken, which can be any token input to column 0 to 6 on the board unless the column is full. For example, at the very beginning, we would expand the root node (initial board state) since that is our only node and it has not yet been expanded.
3. **Simulation**. From the last node reached, simulate a game until its terminal state.
    1. This is usually achieved by picking random nodes. For our case, we use the neural network’s predictions to make a next move.
4. **Backpropagation**. The result of the game is backpropagated up the visited nodes.
    1. Add 1 to the visit count.
    2. Add 1 to reward if game is won, 0 if tied. Every other node will get an inverted reward (-1) because it represents an opponent move.

## Neural Network Architecture

The neural network accepts as input a 6 x 7 x 2 array, where the first binary slice is a representation of the tokens of the player that needs to make the next move (current player), while the second slice represents the position of the opponent’s tokens on the 6 x 7 board. The neural network outputs a value and a policy. The value is a scalar from 0 to 1, representing the probability of the current player winning. The policy is a vector of length 7, where the value at the ith position represents the “goodness” of placing a token in the ith column.

The neural network is composed of a convolution layer, many residual layers, and then a split into a value and policy head. We used [David Foster’s implementation](https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning).

## Neural Network Training

Here is the pseudo code for training the neural network.

1. Initialize deep residual neural network with two heads, one for value and one for policy.
2. Simulate self-play MCTS games with multiprocessing using the initial neural network.
3. For each training iteration:
    1. Collect training data generated by the self-play games.
    2. Start new MCTS games by using multiprocessing.
    3. At the same time, train network over sampled training data.

The most difficult part was tuning all the parameters:

- number of sampling moves per game (using softmax) before switching to deterministic choices,
- number of MCTS simulations to pick the next move in the game,
- number of games to generate per iteration,
- amount of noise to include in MCTS,
- batch size and number of epochs per training iteration,
- size of game buffer (number of games to keep for training data sampling),
- and the learning rate.

We also had to consider our limited resources, as we had no GPU and only 4 cores. Another issue was with operating systems. Multiprocessing with TensorFlow worked reliably on a Unix operating system, but would not work on a Windows operating system. We were thus relegated to one iMac.

Initially, I tried training it with the following parameters:

```python
num_sampling_moves = 12
num_simulations = 200
num_games = 4

root_dirichlet_alpha = 0.4
exploration_factor = 0.2

epochs = 3
batch_size = 32
sample_size = 256
window_size = 16
iterations = 1080
learning_rate = 0.2
```

This resulted in a loss that hovered around 2. The model learned how to create vertical stacks to win, but could not block the opponent’s moves or create a new stack if the opponent blocks the first stack.

![losses.png](images/connect-four/losses.png)

Next, I tried adding a scheduled learning rate, increased the number of games per iteration, and added more noise to the self-play games for the first 100 iterations.

```python
num_sampling_moves = 10
num_simulations = 110
num_games = 16

root_dirichlet_alpha = 0.3
exploration_factor = 0.25

epochs = 20
batch_size = 32
sample_size = 480
window_size = 48

iterations = 2000
random_iterations = 100 # add noise
learning_rate_schedule = {
    0: 2e-1,
    50: 2e-2,
    150: 2e-3,
    250: 2e-4
}
```

This model is currently training.

[^1]: Silver, D., Schrittwieser, J., Simonyan, K. *et al*. Mastering the game of Go without human knowledge. *Nature* **550**, 354–359 (2017). https://doi.org/10.1038/nature24270

[^2]: Silver, D., Hubert T., Schrittwieser, J. *et al*. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play *Science* **362**, 1140-1144 (2018). https://doi.org/10.1126/science.aar6404

[^3]: Czech, J., Korus, P., & Kersting, K. (2020). Monte-Carlo Graph Search for AlphaZero. *arXiv preprint arXiv:2012.11045*.